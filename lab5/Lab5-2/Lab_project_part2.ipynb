{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7PRKfXw99hy"
   },
   "source": [
    "# **Lab Project Part 2 - CNNs for Image Classification**\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY6wdmc299h1"
   },
   "source": [
    "### General Guideline\n",
    "1. **Aim**:\n",
    "    - *Understand  the  basic  Image  Classification/Recognition  pipeline  and  the  data-driven  approach (train/predict stages).*\n",
    "    - *Get used to one of deep learning framework(PyTorch).*\n",
    "2. **Prerequisite**:\n",
    "    - *Familiar with python and relevant packages.*\n",
    "    - *Known the basic knowledge of Convolutional Neural Networks*\n",
    "3. **Guidelines**:\n",
    "    Students should work on the assignments in a group of *three person* for two weeks. Some minor additions and changes might be done during these three weeks. Students will be informed for these changes via Canvas. Any questions regarding the assignment content can be discussed on Piazza. Students are expected to do this assignment in Python and Pytorch, however students are free to choose other tools (like Tensorflow). Your source code and report must be handed in together in a zip file (*ID1_ID2_ID3.zip*) before the deadline. Make sure your report follows these guidelines:\n",
    "    - *The maximum number of pages is 10 (single-column, including tables and figures). Please express your thoughts concisely.*\n",
    "    - *Follow the given script and answer all given questions (in green boxes). Briefly describe what you implemented. Blue boxes are there to give you hints to answer questions.*\n",
    "    - *Analyze your results and discuss them, e.g.*, why algorithm A works better than algorithm B in a certain problem.\n",
    "    - *Tables and figures must be accompanied by a brief description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.*\n",
    "4. **Late submissions** are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs' system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "5. **Plagiarism note**: \n",
    "Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations.\n",
    "\n",
    "### PyTorch versions\n",
    "we assume that you are using the latest PyTorch version(>=1.4)\n",
    "\n",
    "### PyTorch Tutorial & Docs\n",
    "This tutorial aims to make you familiar with the programming environment that will be used throughout the course. If you have experience with PyTorch or other frameworks (TensorFlow, MXNet *etc.*), you can skip the tutorial exercises; otherwise, we suggest that you complete them all, as they are helpful for getting hands-on experience.\n",
    "\n",
    "**Anaconda Environment** If you want to run the notebook locally, we recommend installing *anaconda* for configuring *python* package dependencies, whereas it's also fine to use other environment managers as you like. The installation of anaconda can be found in [anaconda link](https://docs.anaconda.com/anaconda/install/).\n",
    "\n",
    "**Installation** The installation of PyTorch is available at [install link](https://pytorch.org/get-started/locally/) depending on your device and system.\n",
    "\n",
    "**Getting start** The 60-minute blitz can be found at [blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), and and examples are at [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "**Documents** There might be potential unknown functions or classes, you shall look through the official documents website ([Docs](https://pytorch.org/docs/stable/index.html)) and figure them out by yourself. (***Think***: What's the difference between *torch.nn.Conv2d* and *torch.nn.functional.conv2d*?)\n",
    "<!-- You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). In this assignments, we wish you to form the basic capability of using one of the well-known   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2EILNe3JR1b"
   },
   "source": [
    "##  **Introduction**\n",
    "\n",
    "This part of the assignment makes use of Convolutional Neural Networks (CNN). The previous part makes use of hand-crafted features like SIFT to represent images, then trains a classifier on top of them. In this way, learning is a two-step procedure with image representation and learning. The method used here instead *learns* the features jointly with the classification. Training CNNs roughly consists of three parts:  (i) Creating the network architecture, (ii) Reprocessing the data, (iii) Feeding the data to the network, and updating the parameters. Please follow the instruction and finish the below tasks. (**Note:**  **Feel free to change the provided codes**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXQBnhT499h2"
   },
   "source": [
    "## **Session 1: Image Classifiation on CIFAR-100**\n",
    "### 1.1 Install pytorch and run the given codes\n",
    "\n",
    "First of all, you need to install PyTorch and relevant packages. In this session, we will use [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) as the training and testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47K1Kzph99h3"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# referenced codes: https://pytorch.org/tutorials/\n",
    "# referenced codes: http://cs231n.stanford.edu/\n",
    "# referenced codes: https://cs.stanford.edu/~acoates/stl10/\n",
    "######################################################\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optimSh\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import utils\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1781,
     "status": "ok",
     "timestamp": 1665063807401,
     "user": {
      "displayName": "Zehao Xiao",
      "userId": "03533745454963237857"
     },
     "user_tz": -120
    },
    "id": "8AU03smp99h4",
    "outputId": "d6d35ff0-2d0d-482f-f975-a13e0d6204e7"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('apple', 'aquarium_fish', 'baby','bear', 'beaver','bed','bee','beetle','bicycle','bottle', 'bowl','boy','bridge', 'bus','butterfly', 'camel','can','castle','caterpillar','cattle',\n",
    " 'chair','chimpanzee','clock','cloud', 'cockroach','couch', 'cra','crocodile', 'cup','dinosaur','dolphin', 'elephant','flatfish', 'forest', 'fox','girl', 'hamster', 'house','kangaroo','keyboard',\n",
    "'lamp', 'lawn_mower', 'leopard', 'lion','lizard','lobster', 'man','maple_tree','motorcycle', 'mountain', 'mouse','mushroom','oak_tree', 'orange','orchid', 'otter', 'palm_tree','pear', 'pickup_truck','pine_tree',\n",
    "'plain', 'plate', 'poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea', 'seal', 'shark','shrew', 'skunk','skyscraper', 'snail','snake','spider',\n",
    "'squirrel', 'streetcar', 'sunflower','sweet_pepper', 'table','tank','telephone', 'television', 'tiger','tractor','train','trout', 'tulip', 'turtle','wardrobe', 'whale', 'willow_tree','wolf', 'woman','worm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvNbVoMh99h5"
   },
   "source": [
    "####  **` Q1.1: test dataloader and show the images of each class  of CIFAR-100 (3-pts)`**  \n",
    "You need to run and modify the given code and **show** the example images of CIFAR-100, **describe** the classes and images of CIFAR-100. (Please visualize at least one picture for the classes of labels from 0 to 4.) (3-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hhMU6z999h6"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title('Sample images')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1665063813631,
     "user": {
      "displayName": "Zehao Xiao",
      "userId": "03533745454963237857"
     },
     "user_tz": -120
    },
    "id": "7iPwTYDB99h7",
    "outputId": "bb01296b-3ec6-4b9a-a8d1-a46d4163a2bb"
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
    "\n",
    "\n",
    "# Data descriptions\n",
    "print('------- CIFAR100 Descriptions -------')\n",
    "print('# classes = ', len(classes))\n",
    "print(\"Image shape: \",images.shape[1:])\n",
    "print(trainset)\n",
    "print(testset)\n",
    "print(\"List of classes: \\n\", np.vstack(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ6OkCV9LIM9"
   },
   "source": [
    "### 1.2 Architecture understanding\n",
    "\n",
    "In this section, we provide two wrapped classes of architectures defined by *nn.Module*. One is an ordinary two-layer network (*TwolayerNet*) with fully connected layers and ReLu, and the other is a Convolutional Network (*ConvNet*) utilizing the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6a1z2vK99h7"
   },
   "source": [
    "####  **`Q1.2: Architecture understanding. Implement architecture of TwolayerNet and ConvNet (4-pts).`**\n",
    "1. Complement the architecture of *TwolayerNet* class, and complement the architecture of *ConvNet* class using the structure of LeNet-5. (2-*pts*)\n",
    "2. Since you need to feed color images into these two networks, what's the kernel size of the first convolutional layer in *ConvNet*? and how many trainable parameters are there in \"F6\" layer (given the calculation process)? (2-*pts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILrfnDDv99h8"
   },
   "outputs": [],
   "source": [
    "class TwolayerNet(nn.Module):\n",
    "    # assign layer objects to class attributes\n",
    "    # nn.init package contains convenient initialization methods\n",
    "    # http://pytorch.org/docs/master/nn.html#torch-nn-init\n",
    "    def __init__(self,input_size ,hidden_size ,num_classes ):\n",
    "        '''\n",
    "        :param input_size: 3*32*32\n",
    "        :param hidden_size: \n",
    "        :param num_classes: \n",
    "        '''\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5E2a9-lz99h8"
   },
   "outputs": [],
   "source": [
    "### NOTE: For simplicity, you can use nn.tanh as the activation function and output the 1x10 length logits directly (omit the RBF units output).\n",
    "class ConvNet(nn.Module):\n",
    "    # Complete the code using LeNet-5\n",
    "    # reference: https://ieeexplore.ieee.org/document/726791\n",
    "    def __init__(self):\n",
    "        # Input: 32x32\n",
    "        # 1. Layer C1, convolutional layer: \n",
    "        # - in_channels = 1: one 32x32 input image,\n",
    "        # - out_chanels = 6: number of feature maps in the convolutional layer\n",
    "        # - *kernel_size = 5 (<=> kernel_size = (5, 5)): 5x5 neighbourhoods\n",
    "        # - stride = 1 (default): overlapping neighbourhoods\n",
    "        self.C1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        \n",
    "        # 2. Layer S2, subsampling layer:\n",
    "        # - subsampling layer can be replaced by easier, nowadays more commonly used average pooling layer\n",
    "        # - kernel_size = 2: 2x2 neighbourhoods\n",
    "        # - stride = kernel_size (default): non-overlapping neighbourhoods, so stride should equal kernel_size\n",
    "        self.S2 = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # 3. Layer C3, convolutional layer:\n",
    "        # - in_channels = 6: number of feature maps in previous layer\n",
    "        # - out_chanels = 16: number of feature maps in the convolutional layer\n",
    "        # - *kernel_size = 5 (<=> kernel_size = (5, 5)): 5x5 neighbourhoods\n",
    "        # - stride = 1 (default): overlapping neighbourhoods\n",
    "        self.C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        \n",
    "        # 4. Layer S4, subsampling layer:\n",
    "        # - subsampling layer can be replaced by easier, nowadays more commonly used average pooling layer\n",
    "        # - kernel_size = 2: 2x2 neighbourhoods\n",
    "        # - stride = kernel_size (default): non-overlapping neighbourhoods, so stride should equal kernel_size\n",
    "        self.S4 = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # 5. Layer C5, convolutional layer:\n",
    "        # - in_channels = 16: number of feature maps in previous layer\n",
    "        # - out_chanels = 120: number of feature maps in the convolutional layer\n",
    "        # - *kernel_size = 5 (<=> kernel_size = (5, 5)): 5x5 neighbourhoods\n",
    "        # - stride = 1 (default): overlapping neighbourhoods\n",
    "        self.C5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        \n",
    "        # 6. Layer F6, fully-connected (linear) layer:\n",
    "        # - in_features = 120: number of feature maps in previous layer\n",
    "        # - out_features = 84: number of bits in 'bitmap'\n",
    "        self.F6 = nn.Linear(in_features=120, out_features=84)\n",
    "        \n",
    "        # 7. Output layer, fully-connected (linear) layer:\n",
    "        # - in_features = 84: number of bits in 'bitmap' of previous layer\n",
    "        # - out_features = 10: number of classes to detect/predict, i.e., digits 0-9\n",
    "        self.L7 = nn.Linear(in_features=84, out_features=10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # instead of using original LeNet-5 paper's custom intermediate activation function, use scaled* plain tanh everywhere\n",
    "        A = 1.7159  #*tanh scaling\n",
    "        \n",
    "        feed1 = nn.Sequential(self.C1, nn.Tanh())\n",
    "        x = self.S2(A * feed1(x))  # feed through C1 (activate with tanh), scale by A, then subsample through S2\n",
    "        \n",
    "        feed2 = nn.Sequential(self.C3, nn.Tanh())\n",
    "        x = self.S4(A * feed2(x))  # feed through C3 (activate with tanh), scale by A, then subsample through S4\n",
    "        \n",
    "        feed3 = nn.Sequential(self.C5, nn.Tanh())\n",
    "        x = torch.flatten(A * feed3(x))  # feed through C5 (activate with tanh), scale by A, then flatten to feed through F6\n",
    "        \n",
    "        feed4 = nn.Sequential(self.F6, nn.Tanh())\n",
    "        x = A * feed4(x)  # feed through F6 (activate with tanh), scale by A\n",
    "        \n",
    "        feed5 = nn.Sequential(self.L7, nn.Tanh())\n",
    "        x = feed5(x)  # feed through L7 (activate with tanh)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCP1tS7lL8hF"
   },
   "source": [
    "### 1.3 Preparation of training\n",
    "\n",
    "In above section, we use the *CIFAR-100* dataset class from *torchvision.utils* provided by PyTorch. Whereas in most cases, you need to prepare the dataset yourself. One of the ways is to create a *dataset* class yourself and then use the *DataLoader* to make it iterable. After preparing the training and testing data, you also need to define the transform function for data augmentation and optimizer for parameter updating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyc8Ksd99h-"
   },
   "source": [
    "####  **` Q1.3: Preparation of training. Create Dataloader yourself and define Transform, optimizer.(8-pts)`**  \n",
    "1. Complement the *CIFAR100\\_loader* (2-pts)\n",
    "2. Complement *Transform* function and *Optimizer* (2-pts)\n",
    "3. Train the *TwolayerNet* and *ConvNet* with *CIFAR100\\_loader*, *Transform* and *Optimizer* you implemented and compare the results (4-pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkImoAsU99h-"
   },
   "source": [
    "##### *` Complement  CIFAR100_loader()(2-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9Za4VSP99h_"
   },
   "outputs": [],
   "source": [
    "###  suggested reference: https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html?highlight=dataloader\n",
    "# functions to show an image\n",
    "\n",
    "class CIFAR100_loader(Dataset):\n",
    "    def __init__(self,root,train=True,transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): directory with all the images\n",
    "            train (boolean, optional): Optional boolean to indicate if loader should retrieve training data\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        import pickle, os\n",
    "        with open(os.path.join(root, 'train' if train else 'test'), mode='rb') as file: # b is important -> binary\n",
    "            self.data = pickle.load(file, encoding='latin1')        \n",
    "        self.data['data'] = self.data['data'].reshape(len(self.data['data']), 3,32,32).transpose(0,2,3,1)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.tolist()\n",
    "        \n",
    "        target = self.data['fine_labels'][item]\n",
    "        img = self.data['data'][item]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYKtNWW_99h_"
   },
   "source": [
    "##### *` Complement Transform function and Optimizer (2-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2yNqZpZ99h_"
   },
   "outputs": [],
   "source": [
    "root = '/home/olineranum/Documents/CV1/lab/Computer-Vision-1/lab5/data/cifar-100-python'\n",
    "\n",
    "# Consider normalizing the images if the training performs poorly\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding = 4, padding_mode = 'reflect'), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(ConvNet().parameters(), lr = 0.1, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYpJN5Cb99h_"
   },
   "source": [
    "##### *` Train the TwolayerNet and ConvNet with CIFAR100_loader, transform and optimizer you implemented and compare the results (4-pts)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FBozCDk99h-"
   },
   "outputs": [],
   "source": [
    "def valid(net,testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "            100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm7FuYwJ99h-"
   },
   "outputs": [],
   "source": [
    "def valid_class(net,testloader,classes):\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNwBZvfX99h_"
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader,epoch=1):\n",
    "    ###################### Define Loss function and optimizer\n",
    "    \n",
    "    ############################### Training\n",
    "    for epoch in range(epoch):  # loop over the dataset multiple times \n",
    "        loss = 0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch \n",
    "        ################################\n",
    "        # Todo: finish the code\n",
    "        ################################\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxfxa_Rj8X5z"
   },
   "source": [
    "*train TwolayerNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd3ql4B27yEl"
   },
   "outputs": [],
   "source": [
    "\n",
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GySb8UWX8emz"
   },
   "source": [
    "*train ConvNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JL9by9A7yQb"
   },
   "outputs": [],
   "source": [
    "\n",
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQhUzRuQOOSm"
   },
   "source": [
    "### 1.4 Setting up the hyperparameters\n",
    "\n",
    "Some parameters must be set properly before the training of CNNs. These parameters shape the training procedure. They determine how many images are to be processed at each step, how much the weights of the network will be updated, how many iterations will the network run until convergence.  These parameters are called hyperparameters in the machine learning literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhtGVNHIQ8kT"
   },
   "source": [
    "####  **` Q1.4: Setting up the hyperparameters (10-pts)`**  \n",
    "\n",
    "1. Play with ConvNet and TwolayerNet yourself, set up the hyperparameters, and reach the accuracy as high as you can.\n",
    "You can modify the *train*,  *Dataloader*, *transform* and *Optimizer* function as you like. \n",
    "2. You can also modify the architectures of these two Nets. \n",
    "\n",
    " *Let's add 2 more layers in TwolayerNet and ConvNet, and show the results. (You can decide the size of these layers and where to add them.) Will you get higher performances? explain why.*\n",
    "3.  Show the final results and described what you've done to improve the results. Describe and explain the influence of hyperparameters among *TwolayerNet* and *ConvNet*.\n",
    "4. Compare and explain the differences of these two networks regarding the architecture, performances, and learning rates. \n",
    "\n",
    "**Hint:** You can adjust the following parameters and other parameters not listed as you like: *Learning rate, Batch size, Number of epochs, Optimizer, Transform function, Weight decay etc.* You can also change the structure a bit, for instance, adding Batch Normalization layers. Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Puiytn99h_"
   },
   "source": [
    "#### *`Play with convNet and TwolayerNet, set up the hyperparameters and reach the accuracy as high as you can`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROyqHC0199iA"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqQ6-Krp99iA"
   },
   "source": [
    "#### *` test the accuracy of ConvNet `*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciidRXDn99iA"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlAIR45199iA"
   },
   "source": [
    "#### *`test the accuracy of TwolayerNet`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2fMEW-g99iA"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XX8c2fb99iA"
   },
   "source": [
    "## **Session 2:  Finetuning the ConvNet**\n",
    "### 2.1 STL-10 DATASET\n",
    "> The above networks are trained on CIFAR-100, which\n",
    "contains the images of 100 different object categories, each of which has $32\\times32 \\times3$ dimensions. \n",
    "The dataset we use throughout this session is a subset of [STL-10](https://cs.stanford.edu/~acoates/stl10/)\n",
    "with higher resolution and different object classes. So, there is a discrepancy between the previous dataset (CIFAR-100) and the new dataset (STL-10). One solution would be to train the whole network from scratch. However, the number of parameters is too large to be trained properly with such few images. Another way is to use the pre-trained network (on CIFAR-100) and then finetune the network on the new dataset (STL-10) (*e.g.*, use the same architectures in all layers except the output layer, as the number of output classes changes (from 100 to 5)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR1xyOW_sib8",
    "outputId": "57689c51-a2cc-40c7-c6fb-216010d2f13f"
   },
   "outputs": [],
   "source": [
    "# Use the following codes if necessary\n",
    "# referenced codes: https://cs.stanford.edu/~acoates/stl10/\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os, sys, tarfile, errno\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "try:\n",
    "    from imageio import imsave\n",
    "except:\n",
    "    from scipy.misc import imsave\n",
    "\n",
    "print(sys.version_info) \n",
    "\n",
    "# image shape\n",
    "HEIGHT = 96\n",
    "WIDTH = 96\n",
    "DEPTH = 3\n",
    "\n",
    "# size of a single image in bytes\n",
    "SIZE = HEIGHT * WIDTH * DEPTH\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# url of the binary data\n",
    "DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
    "\n",
    "# path to the binary train file with image data\n",
    "DATA_PATH = './data/stl10_binary/train_X.bin'\n",
    "\n",
    "# path to the binary train file with labels\n",
    "LABEL_PATH = './data/stl10_binary/train_y.bin'\n",
    "\n",
    "def read_labels(path_to_labels):\n",
    "    \"\"\"\n",
    "    :param path_to_labels: path to the binary file containing labels from the STL-10 dataset\n",
    "    :return: an array containing the labels\n",
    "    \"\"\"\n",
    "    with open(path_to_labels, 'rb') as f:\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "\n",
    "def read_all_images(path_to_data):\n",
    "    \"\"\"\n",
    "    :param path_to_data: the file containing the binary images from the STL-10 dataset\n",
    "    :return: an array containing all the images\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path_to_data, 'rb') as f:\n",
    "        # read whole file in uint8 chunks\n",
    "        everything = np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "        # We force the data into 3x96x96 chunks, since the\n",
    "        # images are stored in \"column-major order\", meaning\n",
    "        # that \"the first 96*96 values are the red channel,\n",
    "        # the next 96*96 are green, and the last are blue.\"\n",
    "        # The -1 is since the size of the pictures depends\n",
    "        # on the input file, and this way numpy determines\n",
    "        # the size on its own.\n",
    "\n",
    "        images = np.reshape(everything, (-1, 3, 96, 96))\n",
    "\n",
    "        # Now transpose the images into a standard image format\n",
    "        # readable by, for example, matplotlib.imshow\n",
    "        # You might want to comment this line or reverse the shuffle\n",
    "        # if you will use a learning algorithm like CNN, since they like\n",
    "        # their channels separated.\n",
    "        images = np.transpose(images, (0, 3, 2, 1))\n",
    "        return images\n",
    "\n",
    "\n",
    "def read_single_image(image_file):\n",
    "    \"\"\"\n",
    "    CAREFUL! - this method uses a file as input instead of the path - so the\n",
    "    position of the reader will be remembered outside of context of this method.\n",
    "    :param image_file: the open file containing the images\n",
    "    :return: a single image\n",
    "    \"\"\"\n",
    "    # read a single image, count determines the number of uint8's to read\n",
    "    image = np.fromfile(image_file, dtype=np.uint8, count=SIZE)\n",
    "    # force into image matrix\n",
    "    image = np.reshape(image, (3, 96, 96))\n",
    "    # transpose to standard format\n",
    "    # You might want to comment this line or reverse the shuffle\n",
    "    # if you will use a learning algorithm like CNN, since they like\n",
    "    # their channels separated.\n",
    "    image = np.transpose(image, (2, 1, 0))\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_image(image):\n",
    "    \"\"\"\n",
    "    :param image: the image to be plotted in a 3-D matrix format\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def save_image(image, name):\n",
    "    imsave(\"%s.png\" % name, image, format=\"png\")\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the STL-10 dataset\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "        print('Downloaded', filename)\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "def save_images(images, labels):\n",
    "    print(\"Saving images to disk\")\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        label = labels[i]\n",
    "        directory = './img/' + str(label) + '/'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST:\n",
    "                pass\n",
    "        filename = directory + str(i)\n",
    "        print(filename)\n",
    "        save_image(image, filename)\n",
    "        i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "uU0aikuks2Lp",
    "outputId": "77f31a37-ead0-4df3-858f-529b19607948"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use the following codes if necessary\n",
    "\n",
    "# download data if needed\n",
    "download_and_extract()\n",
    "\n",
    "# # test to check if the image is read correctly\n",
    "with open(DATA_PATH) as f:\n",
    "    image = read_single_image(f)\n",
    "    plot_image(image)\n",
    "\n",
    "# # test to check if the whole dataset is read correctly\n",
    "images = read_all_images(DATA_PATH)\n",
    "print(images.shape)\n",
    "\n",
    "# labels = read_labels(LABEL_PATH)\n",
    "print(labels.shape)\n",
    "\n",
    "# # save images to disk\n",
    "# save_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pW0p0R6d99iA"
   },
   "source": [
    "#### **`Q2.1 Create the STL10_Dataset (5-pts)`**\n",
    "In this Session, download STL-10 and extract 5 classes from STL-10 training dataset. The the labels of images will be defined as: \n",
    "\n",
    "`{1: 'car',2:'deer',3:'horse',4:'monkey',5:'truck'}`\n",
    "\n",
    " Extract mentioned 5 classes of images from STL-10. Complement *`STL10_Dataset`* class and match each class with the label accordingly. Hint: You can use the codes above to help to complement *`STL10_Dataset`* class. (5-pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPhepFAq99iA"
   },
   "outputs": [],
   "source": [
    "class STL10_Dataset(Dataset):\n",
    "    def __init__(self,root,train=True,transform = None):\n",
    "        ################################\n",
    "        # Todo: finish the code\n",
    "        ################################\n",
    "\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        ################################\n",
    "        # Todo: finish the code\n",
    "        ################################\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ################################\n",
    "        # Todo: finish the code\n",
    "        ################################\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLyFVEWw3Ge"
   },
   "source": [
    "### 2.2 Fine-tuning ConvNet\n",
    "You should load the pre-trained parameters and modify the output layer of pre-trained ConvNet from 100 to 5. You can either load the pre-trained parameters and then modify the output layer, or change the output layer firstly and then load the matched pre-trained parameters. The examples can be found at [link1](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and [link2](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC3ipaYR99iB"
   },
   "source": [
    "#### **`Q2.2  Finetuning from ConvNet (10-pts)`**\n",
    "1. Load the pre-trained parameters (pretrained on CIFAR-100) and modify the ConvNet. (5-pts)\n",
    "2. Train the model and show the results (settings of hyperparameters, accuracy, learning curve). (5-pts)\n",
    "\n",
    "**Hint**:  Once the network is trained, it is a good practice to understand the feature space by visualization techniques. There are several techniques to visualize the feature space. [**t-sne**](https://lvdmaaten.github.io/tsne/) is a dimensionality reduction method which can help you better understand the feature learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PN_l_WX199iB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jkgXVGt99iB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDUT9jtY99iB"
   },
   "source": [
    "###  **`Bonus (optional)`**\n",
    "Play with the code and try to get a higher accuracy on the test dataset (5 class from STL-10) as high as you can. The only data you can use is from CIFAR-100 and SLT-10. The higher accuracy among all teams can get extra points. Specifically, **1st:** *5-pts*, **2nd and 3rd:** *4-pts*, **4th and 5th:** *3-pts*, **6th and 7th:** *2-pts*, **8th-10th:** *1-pts*. You can adjust the hyperparameters and changing structures. Your strategies should be described and explained in your report.\n",
    "\n",
    "*Please do not use external well-defined networks and please do not add more than 3 additional (beyond the original network) convolutional layers.*\n",
    "\n",
    "**Hints**:\n",
    "*   Data augmentation\n",
    "*   Grid Search\n",
    "*   Freezing early layers\n",
    "*   Modifying Architecture\n",
    "*   Modifying hyperparameters, *etc*.\n",
    "*   [Other advice](https://cs231n.github.io/transfer-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1YsjfP199iB"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Todo: finish the code\n",
    "################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
