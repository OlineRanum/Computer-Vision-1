{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzu4lRc4Rx1a"
   },
   "source": [
    "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F450%2F1*CXZ804tKLPy2hiikJbYH3w.png&f=1&nofb=1\" width=30% ></center>\n",
    "\n",
    "# <center> Assignment 4: Image Alignment and Stitching </center>\n",
    "<center> Computer Vision 1, University of Amsterdam </center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:02.477069Z",
     "start_time": "2022-09-09T15:14:01.955194Z"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1664396385690,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "kUZ6V88wO26-"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:02.487448Z",
     "start_time": "2022-09-09T15:14:02.482524Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664396385977,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "tnj6diy_5wEU"
   },
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"3.4.2\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.19.5\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.3.4\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Plotting paramemters\n",
    "matplotlib.rcParams['figure.figsize']  = (20.0, 10.0)\n",
    "matplotlib.rcParams['axes.grid']       = False\n",
    "matplotlib.rcParams['font.size']       = 30\n",
    "matplotlib.rcParams['axes.labelsize']  = 0.8*plt.rcParams['font.size']\n",
    "matplotlib.rcParams['axes.titlesize']  = 0.9*plt.rcParams['font.size']\n",
    "matplotlib.rcParams['legend.fontsize'] = plt.rcParams['font.size']\n",
    "matplotlib.rcParams['xtick.labelsize'] = 0.5*plt.rcParams['font.size']\n",
    "matplotlib.rcParams['ytick.labelsize'] = 0.5*plt.rcParams['font.size']\n",
    "matplotlib.rcParams['scatter.marker']  = 'o'\n",
    "matplotlib.rcParams['axes.titlepad']   = 20\n",
    "matplotlib.rcParams['axes.labelpad']   = 20\n",
    "matplotlib.rcParams['xtick.major.pad'] ='10'\n",
    "matplotlib.rcParams['ytick.major.pad'] ='10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bz6HHFqKunL"
   },
   "source": [
    "# 1. Image Alignment with SIFT & RANSAC\n",
    "\n",
    "We build a function that takes two images as\n",
    "input and computes the affine transformation between them. The overall scheme is as follows:\n",
    "\n",
    "  1.  Detect interest points in each image.\n",
    "\n",
    "  2.  Characterize the local appearance of the regions around interest\n",
    "      points.\n",
    "\n",
    "  3.  Get the set of supposed matches between region descriptors in each\n",
    "      image.\n",
    "\n",
    "  4.  Perform RANSAC to discover the best transformation between images. RANSAC is performed as follows:\n",
    "\n",
    "  -   Repeat $N$ times:\n",
    "\n",
    "  -   Pick $P$ matches at random from the total set of matches $T$.\n",
    "\n",
    "  -   Construct a matrix $A$ and vector $b$ using the $P$ pairs of points and find affine transformation parameters $(m1, m2, m3, m4, t1, t2)$ by solving the equation $Ax = b$. Such equation can be solved using the pseudo-inverse: $x = (A^T A)^{-1} A^T b$, or packages of Numpy in Python.\n",
    "\n",
    "  - Using the transformation parameters, transform the locations of all $T$ points in image1. If the transformation is correct, they should lie close to their counterparts in image2. Plot the two images side by side with a line connecting the original $T$ points in image1 and transformed $T$ points over image2.\n",
    "      \n",
    "  - Count the number of inliers, where inliers are defined as the number of transformed points from image1 that lie within a radius of $10$ pixels of their pair in image2.\n",
    "\n",
    "  - If this count exceeds the best total so far, we save the transformation parameters and the set of inliers.\n",
    "\n",
    "  - End repeat.\n",
    "\n",
    "5. Transform image1 using this final set of transformation parameters. If you display this image, you should find that the pose of the object in the scene should correspond to its pose in image2. To transform the image, we use our own function based on **nearest-neighbor interpolation**. We then compute the affine transformation using the OpenCV built-in function `cv2.warpAffine` and compare the results.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5EmEVDZNx9K"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "### 1.1 Keypoint matching\n",
    "\n",
    "The following contains a function that takes two image pairs, and return the keypoint matching between them using the built-in cv2 keypoint and matching functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:02.985171Z",
     "start_time": "2022-09-09T15:14:02.499783Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1664396387077,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "dfsTvoUsP_dC",
    "outputId": "17c2c47d-f8f7-4c10-ab0a-ebc3be5f4258"
   },
   "outputs": [],
   "source": [
    "img1_path = \"street1.png\" \n",
    "img2_path = \"street2.png\"\n",
    "\n",
    "# Open images\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "\n",
    "# Note: OpenCV uses BGR instead of RGB\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) \n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display images\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "plt.suptitle('Building affine transformations')\n",
    "ax[0].imshow(img1)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Image 1')\n",
    "ax[1].imshow(img2)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Image 2')\n",
    "txt= r\"Figure 1.1: The two images used as a basis to develop the affine transformation\"\n",
    "plt.figtext(0.5, 0.2, txt, wrap=True, horizontalalignment='center', fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:02.999868Z",
     "start_time": "2022-09-09T15:14:02.989677Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1664396387078,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "l8qqxlZ12B-M"
   },
   "outputs": [],
   "source": [
    "def keypoint_matching(image1, image2):\n",
    "    \"\"\"\n",
    "    Given two input images, find and return the matching keypoints.\n",
    "    Arguments:\n",
    "    image1: the first image (in RGB)\n",
    "    image2: the second image (in RGB)\n",
    "    Returns: \n",
    "    The keypoints of image1, the keypoints of image2 and the matching\n",
    "    keypoints between the two images\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\nFinding matching features...')\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Initiate the SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    # Find the keypoints and descriptors with SIFT\n",
    "    keypoints_1, des_1 = sift.detectAndCompute(image1,None)\n",
    "    keypoints_2, des_2 = sift.detectAndCompute(image2,None)\n",
    "    \n",
    "    # Draw images of keypoints\n",
    "    img = cv2.drawKeypoints(image1, keypoints_1, image1, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite('sift_keypoints_1.jpg',img)\n",
    "    \n",
    "    img = cv2.drawKeypoints(image2, keypoints_2, image2, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite('sift_keypoints_2.jpg',img)\n",
    "    \n",
    "    # Create BFMatcher object\n",
    "    bf = cv2.BFMatcher(crossCheck=True)\n",
    "    \n",
    "    # Match Descriptors\n",
    "    matches = bf.match(des_1,des_2)\n",
    "    \n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    \n",
    "    print(\"Number of keypoints in img1:        \", len(keypoints_1))\n",
    "    print(\"Number of keypoints in img2:        \", len(keypoints_2))\n",
    "    print(\"Number of keypoints after matching: \", len(matches), \"\\n\")\n",
    "\n",
    "    return keypoints_1, keypoints_2, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:03.584052Z",
     "start_time": "2022-09-09T15:14:03.010719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1664396387401,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "2cOemdi7iTJz",
    "outputId": "760b0040-8272-4bea-8c2a-ca6566d2713c"
   },
   "outputs": [],
   "source": [
    "# Find and match key points\n",
    "keypoints_1, keypoints_2, matches = keypoint_matching(img1,img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0C2YSEbPmpL"
   },
   "source": [
    "### Question 1.2\n",
    "We take a random subset (with set size set to 10) of all matching points, and plot them on the image. We connect matching pairs with lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:03.992587Z",
     "start_time": "2022-09-09T15:14:03.593876Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1664396388383,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "AOCsvlxWvLMn",
    "outputId": "04075e06-cc8c-442a-b009-a4d1833395a9"
   },
   "outputs": [],
   "source": [
    "# Extract 10 random matches to plot\n",
    "random_matches = [matches[i] for i in random.sample(range(0, len(matches)), 10)]\n",
    "\n",
    "# Plot random selection of key point matches\n",
    "img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, random_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.title('10 Randomly selected matches between the two images')\n",
    "txt= r\"Figure 1.2: 10 matches drawn with lines inbetween the corresponding points. The matches are calculated using the built-in keypoint finder and matcher of cv2.\"\n",
    "plt.figtext(0.5, 0.2, txt, wrap=True, horizontalalignment='center', fontsize=15)\n",
    "plt.imshow(img3),plt.show()\n",
    "cv2.imwrite('sift_keypoints_3.jpg',img3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0_Jxh1VQfLj"
   },
   "source": [
    "### Question 1.3\n",
    "Create a function that performs the RANSAC algorithm as explained above. The function should return the best transformation found. For visualization, show the transformations from image1 to image2 and from image2 to image1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:04.008550Z",
     "start_time": "2022-09-09T15:14:03.994507Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664396388383,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "aTAsTw4vKjs4"
   },
   "outputs": [],
   "source": [
    "# Self-designed functions 'get_grid' and 'affine_transform' for ranasc algorithm.\n",
    "\n",
    "def get_grid(w, h):\n",
    "    # Generates HOMOGENEOUS coordinates of an image in a grid-like fashion.\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w, h: the width and height of the image\n",
    "    Returns: \n",
    "    HOMOGENEOUS coordinates\n",
    "    \"\"\"\n",
    "    dst_y, dst_x = np.indices((h, w))\n",
    "    dst_grid = np.stack((dst_x.ravel(), dst_y.ravel(), np.ones(dst_y.size)))\n",
    "    return dst_grid\n",
    "\n",
    "\n",
    "def affine_transform(img, mat, warp='forward'):\n",
    "    #Applies the affine transformation defined in mat to the image contained in img.\n",
    "    #Works in homogeneous coordinates.\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    img: the first/second image (img1 or img2)\n",
    "    mat: transformation matrix\n",
    "    warp: forward or inverse warping\n",
    "    Returns:\n",
    "    transformed image\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Make grid to from the image coordinates\n",
    "    grid = get_grid(w, h)\n",
    "    x_orientation, y_orientation = grid[0], grid[1]\n",
    "        \n",
    "    warped_grid = np.round(mat@grid).astype(np.int)\n",
    "    \n",
    "    if warp =='forward':\n",
    "        # Apply transformation on grid with mat\n",
    "        warped_grid = np.round(mat@grid).astype(np.int)\n",
    "\n",
    "        # Get grid lines\n",
    "        x_warped, y_warped = warped_grid[0,:], warped_grid[1,:]\n",
    "\n",
    "\n",
    "        # Get pixels within image boundary\n",
    "        indices = np.where((x_warped >= 0) & (x_warped < w) &\n",
    "                           (x_warped >= 0) & (y_warped < h))\n",
    "\n",
    "        # Get orientation vectors within boundaries\n",
    "        src_x, src_y = x_orientation[indices].astype(int), y_orientation[indices].astype(int)\n",
    "        dst_x, dst_y = x_warped[indices].astype(int), y_warped[indices].astype(int)\n",
    "\n",
    "\n",
    "        # Map the pixel to new location\n",
    "        dst_map = np.zeros_like(img)\n",
    "        dst_map[dst_y, dst_x] = img[src_y,src_x]\n",
    "        return dst_map\n",
    "\n",
    "    if warp == 'inverse':\n",
    "        inv_warped_grid = np.round(np.linalg.inv(mat)@grid).astype(np.int)        \n",
    "        inv_x_warped, inv_y_warped = inv_warped_grid[0,:], inv_warped_grid[1,:]\n",
    "        \n",
    "        # Get pixels within image boundary\n",
    "        indices = np.where((inv_x_warped >= 0) & (inv_x_warped < w) &\n",
    "                       (inv_y_warped >= 0) & (inv_y_warped < h))\n",
    "        \n",
    "        # Get orientation vectors within boundaries\n",
    "        \n",
    "        src_x, src_y = x_orientation[indices].astype(int), y_orientation[indices].astype(int)\n",
    "        dst_x, dst_y = inv_x_warped[indices].astype(int), inv_y_warped[indices].astype(int)\n",
    "    \n",
    "        # Map the pixel to new location\n",
    "        dst_map = np.zeros_like(img)\n",
    "        dst_map[src_y,src_x] = img[dst_y, dst_x]\n",
    "        \n",
    "        return dst_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:04.041570Z",
     "start_time": "2022-09-09T15:14:04.014401Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664396388384,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "KEAKbuPKQURC"
   },
   "outputs": [],
   "source": [
    "def ransac(kp1, kp2, matches, N, print_vals = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      kp1: the keypoints of image1, \n",
    "      kp2: the keypoints of image2\n",
    "      matches: the matching keypoints between the two images\n",
    "      N: number of iterations\n",
    "    Returns: \n",
    "      the best transformation matrix\n",
    "    \"\"\"\n",
    "    # Determine number of randomly selected P values from set T\n",
    "    P = 3\n",
    "    best_inliers = 0\n",
    "    best_matrix = None\n",
    "    \n",
    "\n",
    "    \n",
    "    # Run estimations N-times\n",
    "    for i in range(N):\n",
    "        # 1. Pick P random matches from T \n",
    "        random_matches = [matches[i] for i in random.sample(range(0, len(matches)), P)]\n",
    "        \n",
    "        # Obtain x, y, x_prime, y_prime\n",
    "        \n",
    "        A = np.empty((0, 6))\n",
    "        b = np.empty((0, 1))\n",
    "        for mat in random_matches: \n",
    "            # Get the matching keypoints for each of the images\n",
    "            img1_idx = mat.queryIdx\n",
    "            img2_idx = mat.trainIdx\n",
    "\n",
    "            # Get the coordinates\n",
    "            x, y = kp1[img1_idx].pt\n",
    "            x_prime, y_prime = kp2[img2_idx].pt\n",
    "            A = np.vstack((A, np.array([[x, y, 0, 0, 1, 0], [0, 0, x, y, 0, 1]])))\n",
    "            b = np.vstack((b, np.array([[x_prime], [y_prime]])))\n",
    "        \n",
    "        m1, m2, m3, m4, t1, t2 = (np.linalg.pinv(A)@b).T[0]\n",
    "        \n",
    "        # 4. Transform the location of all T points in image 1\n",
    "        affine_transform_mat = np.array([[m1, m2, t1], [m3, m4, t2], [0, 0, 1]])\n",
    "        \n",
    "        temp_inliers = 0\n",
    "        for match in matches:\n",
    "            point_index = match.queryIdx\n",
    "            point2_index = match.trainIdx\n",
    "            point_x, point_y = kp1[point_index].pt\n",
    "            homogeneous_point = np.array([[point_x], [point_y], [1]])\n",
    "            transformed_homogeneous_point = affine_transform_mat@homogeneous_point\n",
    "            transformed_point = np.array([\n",
    "                transformed_homogeneous_point[0]/transformed_homogeneous_point[2],\n",
    "                transformed_homogeneous_point[1]/transformed_homogeneous_point[2]]).T[0]\n",
    "            \n",
    "            # 5. Count inliners \n",
    "            dist = np.linalg.norm(transformed_point-np.array(kp2[point2_index].pt))\n",
    "            if dist < 10:\n",
    "                temp_inliers += 1\n",
    "        \n",
    "        # Compare to previous itteration\n",
    "        if temp_inliers > best_inliers:\n",
    "            best_inliers = temp_inliers\n",
    "            best_matrix = affine_transform_mat\n",
    "\n",
    "\n",
    "    if print_vals:\n",
    "        print(\"Total number of matches: \", len(matches))\n",
    "        print(\"Inliers found:           \", best_inliers)\n",
    "        print(\"Outliers removed:        \", len(matches) - best_inliers)\n",
    "\n",
    "    return best_matrix, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:04.052617Z",
     "start_time": "2022-09-09T15:14:04.045925Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664396388384,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "wtkuqoQTgPk9"
   },
   "outputs": [],
   "source": [
    "def visualization(img, best_matrix):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      img: the first/second image (img1 or img2)\n",
    "      best_matrix: the best transformation matrix\n",
    "    \"\"\"  \n",
    "    # Visualize/export a comparison: \"Forward warping\", \"Inverse warping\", \n",
    "    # \"OpenCV warping\", \"Original image\". Refer to the output of the next\n",
    "    # cell to see the expected output.\n",
    "\n",
    "    # Read the image\n",
    "    rows, cols = img.shape[:2]\n",
    "\n",
    "    # Apply the affine transformation using cv2.warpAffine()\n",
    "    dst_cv2 = cv2.warpAffine(img, best_matrix[:2,:], (cols,rows))\n",
    "    \n",
    "    # Calculate forward \n",
    "    dst_forward = affine_transform(img1, best_matrix)\n",
    "\n",
    "    # Calculate forward \n",
    "    dst_inverse = affine_transform(img1, best_matrix, warp = 'inverse')\n",
    "\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.suptitle('Image warping with 3 different methods')\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(img)\n",
    "    plt.title('A.: Original image')\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(dst_cv2)\n",
    "    plt.title('B.: Open CV warping')\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(dst_forward)\n",
    "    plt.title('C.: Forward warping')\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(dst_inverse)\n",
    "    plt.title('D.: Inverse warping')\n",
    "    txt= r\"Figure 1.3: Image warping of image 1 using three different methods.\"\n",
    "    plt.figtext(0.5, 0.3, txt, wrap=True, horizontalalignment='center', fontsize=15)\n",
    "    plt.savefig('wrapping.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:14:05.654873Z",
     "start_time": "2022-09-09T15:14:04.060183Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "executionInfo": {
     "elapsed": 3860,
     "status": "ok",
     "timestamp": 1664396392238,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "37ELWqN-pcup",
    "outputId": "e3c7b928-8bed-4344-e03b-14ce2436ccdd"
   },
   "outputs": [],
   "source": [
    "N_iterations = 50 # experiment with this value!\n",
    "best_matrix, inliers = ransac(keypoints_1,keypoints_2,matches, N_iterations)\n",
    "img1 = cv2.imread(img1_path)\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) \n",
    "visualization(img1,best_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Comparing the results\n",
    "\n",
    "In the above we apply the affine transformation to the original image 1 using three methods: the openCV built-in warping function, and our self implemented forward and inverse warping functions using SIFT and RANSAC. \n",
    "\n",
    "When computing the forward warping transformation (Figure 1.3.C) we can clearly observe dark structures, or empty pixel slots, which are not present in the OpenCV implementation or the rotated version of the original image. These effects occur due to aliasing, artifacts due to under-sampling, and holes. It can happen when performing a forward warping that some pixels are not taken into account in the new mapping, as their exact positioning under the warping will be located between the pixels of the warped grid. \n",
    "\n",
    "One way to correct for this effect is 'splatting', which is to distribute color among the neighboring pixels. Another wasy to implement approach to prevent this aliasing is to perform an inverse transformation, i.e. to use the inverted version of the parameter matrix A. In this sense, we resample the source image upon the warped grid, using a nearest neighbor approach instead of an interpolation mechanism. As can be observed in figure 1.3.D this methodology is successful in implementing a transformation that is at surface level equivalent to the openCV implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mQUcN9KTj2R"
   },
   "source": [
    "## Question 2\n",
    "Based on the results, answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLP980JRSZas"
   },
   "source": [
    "<a name=\"q2.1\"></a>\n",
    "### Question 2.1\n",
    "How many matches do we need to solve an affine transformation which can be formulated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x'\\\\y'\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "m_1 & m_2\\\\\n",
    "m_3 & m_4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\y\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "t_1\\\\t_2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The equation above can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x&y&0&0&1&0\\\\\n",
    "0&0&x&y&0&1\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "m_1\\\\\n",
    "m_2\\\\\n",
    "m_3\\\\\n",
    "m_4\\\\\n",
    "t_1\\\\\n",
    "t_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x'\\\\y'\\end{bmatrix}\n",
    "$$\n",
    "or, alternatively:\n",
    "$$\n",
    "Ax=b, \\;\n",
    "A = \\begin{bmatrix}\n",
    "x&y&0&0&1&0\\\\\n",
    "0&0&x&y&0&1\\end{bmatrix}, \\;\n",
    "x = \\begin{bmatrix}\n",
    "m_1\\\\\n",
    "m_2\\\\\n",
    "m_3\\\\\n",
    "m_4\\\\\n",
    "t_1\\\\\n",
    "t_2\n",
    "\\end{bmatrix}, \\;\n",
    "b = \\begin{bmatrix}\n",
    "x'\\\\y'\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41ovCQWOSyCO"
   },
   "source": [
    "**ADD YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIw3Fk92UWrq"
   },
   "source": [
    "### 2.2: The number of iterations in average needed to find good transformation parameters\n",
    "\n",
    "In the section below we run E (E = 15) experiments of calulacting the max number of inliers after N iterations, allowing N to run from 1 to 50 for each experiment.  The values of E and N are found by systematic trial and error. We then take the average of all experiments $E_i$ and plot them as a function of N.\n",
    "\n",
    "From looking at Figure 2.2.1 we can observe that the average max number of inliers start to converge around N = 20 to a value of approximatly 800, which appears to be the convergence value for these set of matches. As such, it would be good practice to select a value slightly higher than N = 20 itterations to find good transformation parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fw4PAWPFUh4g"
   },
   "source": [
    "**ADD YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iterations = 50 # experiment with this value!\n",
    "N_vec = np.arange(1, N_iterations + 1, 1)\n",
    "E_experiments = 15\n",
    "\n",
    "Inliers_mat = np.zeros((E_experiments, len(N_vec)))\n",
    "\n",
    "for i in range(E_experiments):\n",
    "    for m in N_vec:\n",
    "        best_matrix, best_inlier_ = ransac(keypoints_1,keypoints_2,matches, m, print_vals = False)\n",
    "        Inliers_mat[i,m-1] = best_inlier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.title('Convergence of average number of max inliers')\n",
    "plt.plot(N_vec, np.mean(Inliers_mat, axis = 0))\n",
    "plt.xlabel('Number of itterations N')\n",
    "plt.ylabel('Average number of inliers after N itterations')\n",
    "txt= r\"Figure 2.2.1: The average number of inliers after \" + str(E_experiments) +\" experiments with N $\\in$ [\" + str(N_vec[0]) + \", \" + str(N_vec[-1])+ \"] itterations, illustrating the convergence values of the max number of inliers\"\n",
    "plt.figtext(0.5, -0.02, txt, wrap=True, horizontalalignment='center', fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ODiE_XiUCwx"
   },
   "source": [
    "____\n",
    "\n",
    "# Image Stitching (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8i6ofe_r2x7"
   },
   "source": [
    "In this practice, you will write a function that takes two images as input and stitch them together. The method described in the previous section will be used to stitch two images together by transforming one of them to the coordinate space of the other. You will work with supplied images *left.jpg* and *right.jpg*. The overall scheme can be summarized as follows:\n",
    "\n",
    "1.   As in previous task you should first find the best transformation between input images.\n",
    "\n",
    "2.   Then you should estimate the size of the stitched image.\n",
    "\n",
    "3.   Finally, combine the *left.jpg* with the transformed *right.jpg* into one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL3DmE2EtElm"
   },
   "source": [
    "## Question 3\n",
    "### Question 3.1 \n",
    "Create a function that takes an image pair as input, and return the stitched version.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:16:03.135975Z",
     "start_time": "2022-09-09T15:16:03.114334Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1664396392239,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "AN8N4BWIyJyB"
   },
   "outputs": [],
   "source": [
    "def stitchImages(img1, img2, N):\n",
    "    \"\"\"\n",
    "    Given two input images, return the stitched image.\n",
    "    Arguments:\n",
    "    img1: the first image (in RGB)\n",
    "    img2: the second image (in RGB)\n",
    "    Returns: \n",
    "    The keypoint matchings between the two image\n",
    "    \"\"\"\n",
    "    \n",
    "    #1. Find the best transformation.\n",
    "    \n",
    "    # Find and match key points\n",
    "    keypoints_1, keypoints_2, matches = keypoint_matching(img1,img2)\n",
    "    \n",
    "    # Extract 10 random matches to plot\n",
    "    random_matches = [matches[i] for i in random.sample(range(0, len(matches)), 10)]\n",
    "\n",
    "    # Now plot them. Hint: for generating the plot, you can use cv2.drawMatches()\n",
    "\n",
    "    img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, random_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.imshow(img3),plt.show()\n",
    "    cv2.imwrite('sift_keypoints_3_bus.jpg',img3)\n",
    "    \n",
    "    best_matrix = ransac(keypoints_1,keypoints_2,matches, N_iterations)\n",
    "    \n",
    "    # Apply transformation\n",
    "    dst_forward = affine_transform(img1, best_matrix, warp = 'inverse')\n",
    "    # Display the image\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1)\n",
    "    plt.title('Original image')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(dst_forward)\n",
    "    plt.title('Open CV warping')\n",
    "    plt.show()\n",
    "    \n",
    "    M, N = img2.shape[:2]\n",
    "\n",
    "    dst_forward[0:M, 0:N, :] = img2\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.imshow(dst_forward); plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: 2. Estimate the size of the stitched image.\n",
    "    # Hint: Calculate the transformed coordinates of corners of the *right.jpg*\n",
    "\n",
    "    # TODO: 3. Combine the *left.jpg* with the transformed *right.jpg* into one image.\n",
    "\n",
    "    # ================\n",
    "    # Your code here\n",
    "    # ================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:19:07.229705Z",
     "start_time": "2022-09-09T15:19:03.812314Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1664396393181,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "ls2Z_dGxyBOM",
    "outputId": "394b6371-f379-4a11-9629-ddecd92957b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img1_path = \"left.jpg\"\n",
    "img2_path = \"right.jpg\"\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "\n",
    "# Note: OpenCV uses BGR instead of RGB\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) \n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Stitch images\n",
    "N_iterations = 50 # Select based on your previous findings.\n",
    "stitchedImage = stitchImages(img1, img2, N_iterations)\n",
    "\n",
    "#plt.imshow(stitchedImage)\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKxyp4jztk0B"
   },
   "source": [
    "### Question 3.2\n",
    "Visualize the stitched image alongside with the image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T15:22:16.642076Z",
     "start_time": "2022-09-09T15:22:16.276518Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1664396393183,
     "user": {
      "displayName": "Weijie Google Wei",
      "userId": "12483427008926387009"
     },
     "user_tz": -120
    },
    "id": "Ncr-Vrdetnhr",
    "outputId": "08accfee-721b-4cdf-e78d-b8fb28fa5f77"
   },
   "outputs": [],
   "source": [
    "# TODO: Visualize the stitched image alongside with the image pair.\n",
    "\n",
    "# ================\n",
    "# Your code here\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "224f13b0336c07d93cf75d08c12c425a966b6415c4707bdda15bc09c162f9d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
